---
title: Paper Dump
feature_text: |
  ## My Paper Reading List
excerpt: |
    Paper need to read
categories:
- Academic
feature_image: "https://picsum.photos/2560/600?image=872"
---


## 经典之作
- Transformer [Attention Is All You Need](https://arxiv.org/abs/1706.03762) [code](https://paperswithcode.com/paper/attention-is-all-you-need)
- GPT [Improving Language Understanding by Generative Pre-Training](https://paperswithcode.com/paper/improving-language-understanding-by)
- GPT-3 [Language Models are Few-Shot Learners](https://paperswithcode.com/paper/language-models-are-few-shot-learners)
- ViLT [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)

## Papers from Prof. Yi Ma's talk
- [CTRL: Closed-Loop Transcription to an LDR via Minimaxing Rate Reduction](https://www.mdpi.com/1099-4300/24/4/456)
- effectively alleviate catastrophic forgetting [Incremental Learning of Structured Memory via Closed-Loop Transcription](https://arxiv.org/abs/2202.05411)
- explainable AI [ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction](https://arxiv.org/abs/2105.10446)
- unsupervised learning [Unsupervised Learning of Structured Representations via Closed-Loop Transcription](https://arxiv.org/abs/2210.16782)